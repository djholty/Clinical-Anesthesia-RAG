name: CI - Test and Lint

on:
  push:
    branches: [ main, dev, david_work ]
  pull_request:
    branches: [ main, dev ]

jobs:
  test:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Free Disk Space
      uses: jlumbroso/free-disk-space@main
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
    
    - name: Cache dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install --no-cache-dir -r requirements.txt
        pip install --no-cache-dir pytest flake8 black
    
    - name: Clean up pip cache
      run: |
        pip cache purge || true
        rm -rf ~/.cache/pip || true
    
    - name: Lint with flake8
      run: |
        # Stop the build if there are Python syntax errors or undefined names
        flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics
        # Exit-zero treats all errors as warnings
        flake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics
      continue-on-error: true
    
    - name: Check code formatting with black
      run: |
        black --check --diff .
      continue-on-error: true
    
    - name: Run basic tests
      run: |
        # Add your test commands here
        python -c "import app.main; print('✓ Main app imports successfully')"
        python -c "import app.rag_pipeline; print('✓ RAG pipeline imports successfully')"
        python -c "import app.monitoring; print('✓ Monitoring module imports successfully')"
      continue-on-error: true
    
    - name: Check if evaluation script runs
      run: |
        python -c "from david_work_files.evaluate_rag import load_questions; print('✓ Evaluation script imports successfully')"
      continue-on-error: true

  # Future: Add evaluation job that runs on a subset of questions
  # evaluate:
  #   runs-on: ubuntu-latest
  #   needs: test
  #   steps:
  #     - uses: actions/checkout@v3
  #     - name: Run evaluation on test set
  #       run: python david_work_files/evaluate_rag.py --test-mode

